FANN_FLO_2.1
num_layers=4
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=1.00000004749745130000e-003
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000005960464480000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=3 11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (3, 3, 5.00000000000000000000e-001) (3, 3, 5.00000000000000000000e-001) (3, 3, 5.00000000000000000000e-001) (3, 3, 5.00000000000000000000e-001) (3, 3, 5.00000000000000000000e-001) (3, 3, 5.00000000000000000000e-001) (3, 3, 5.00000000000000000000e-001) (3, 3, 5.00000000000000000000e-001) (3, 3, 5.00000000000000000000e-001) (3, 3, 5.00000000000000000000e-001) (0, 3, 5.00000000000000000000e-001) (11, 3, 5.00000000000000000000e-001) (11, 3, 5.00000000000000000000e-001) (11, 3, 5.00000000000000000000e-001) (11, 3, 5.00000000000000000000e-001) (11, 3, 5.00000000000000000000e-001) (11, 3, 5.00000000000000000000e-001) (11, 3, 5.00000000000000000000e-001) (11, 3, 5.00000000000000000000e-001) (11, 3, 5.00000000000000000000e-001) (11, 3, 5.00000000000000000000e-001) (0, 3, 5.00000000000000000000e-001) (11, 3, 5.00000000000000000000e-001) (0, 3, 5.00000000000000000000e-001) 
connections (connected_to_neuron, weight)=(0, -8.05943870544433590000e+000) (1, 7.58484697341918950000e+000) (2, -3.66569113731384280000e+000) (0, -6.28228340148925780000e+001) (1, 8.57139396667480470000e+000) (2, -4.13283443450927730000e+000) (0, -8.71676325798034670000e-001) (1, -1.51810538768768310000e+000) (2, -2.67552495002746580000e-001) (0, -6.47583866119384770000e+000) (1, 4.05644369125366210000e+000) (2, -1.91840410232543950000e+000) (0, -3.15685915946960450000e+000) (1, 4.46946668624877930000e+000) (2, 3.11679220199584960000e+000) (0, 5.59475135803222660000e+000) (1, -6.29948902130126950000e+000) (2, -3.61024951934814450000e+000) (0, 6.06673860549926760000e+000) (1, -2.04978313446044920000e+001) (2, -3.36367511749267580000e+000) (0, 4.05695796012878420000e-001) (1, -1.02226936817169190000e+000) (2, -4.32024812698364260000e+000) (0, 1.93962562084198000000e+000) (1, -2.75224828720092770000e+000) (2, -3.57964968681335450000e+000) (0, 1.24982309341430660000e+000) (1, -2.27835321426391600000e+000) (2, 3.26918214559555050000e-001) (3, 3.32885813713073730000e+000) (4, 4.90248727798461910000e+000) (5, -2.64626383781433110000e+000) (6, 2.04886174201965330000e+000) (7, 1.56649994850158690000e+000) (8, 1.39278185367584230000e+000) (9, 1.73043787479400630000e-001) (10, 1.61722040176391600000e+000) (11, -1.27809500694274900000e+000) (12, -2.46402263641357420000e+000) (13, -5.12581777572631840000e+000) (3, -1.50659799575805660000e+000) (4, -4.56047058105468750000e+000) (5, 1.89101576805114750000e+000) (6, 1.99487590789794920000e+000) (7, 3.09084010124206540000e+000) (8, -3.89840793609619140000e+000) (9, -8.53647470474243160000e-001) (10, 1.40921473503112790000e+000) (11, 1.71724929809570310000e+001) (12, 1.46728479862213130000e+000) (13, -1.89766779541969300000e-001) (3, 3.45110177993774410000e-001) (4, 2.36732864379882810000e+000) (5, -3.09766501188278200000e-001) (6, -1.84017801284790040000e+000) (7, -4.39047050476074220000e+000) (8, 3.09881544113159180000e+000) (9, 2.19188570976257320000e+000) (10, -3.39889109134674070000e-001) (11, -1.56741638183593750000e+001) (12, -1.30115818977355960000e+000) (13, 1.68700516223907470000e+000) (3, 5.87358188629150390000e+000) (4, 3.22209310531616210000e+000) (5, 1.66481530666351320000e+000) (6, 1.82172393798828120000e+000) (7, -1.03876090049743650000e+000) (8, 3.22754383087158200000e+000) (9, 6.26896476745605470000e+000) (10, 1.51951044797897340000e-001) (11, -2.06373238563537600000e+000) (12, -1.23523950576782230000e+000) (13, -4.61788654327392580000e+000) (3, 7.23809432983398440000e+000) (4, 9.50327515602111820000e-001) (5, 4.15743201971054080000e-001) (6, 1.04480636119842530000e+000) (7, -8.18247020244598390000e-001) (8, 1.68655931949615480000e+000) (9, 4.50241379439830780000e-002) (10, 8.52277457714080810000e-001) (11, -5.61315488815307620000e+000) (12, 1.22247087955474850000e+000) (13, -3.52528333663940430000e+000) (3, 2.25492882728576660000e+000) (4, 2.85954570770263670000e+000) (5, 4.80825513601303100000e-001) (6, 7.49536931514739990000e-001) (7, -9.08246803283691410000e+000) (8, 3.18846130371093750000e+000) (9, 5.61668205261230470000e+000) (10, -2.79126334190368650000e+000) (11, -7.18871259689331050000e+000) (12, 4.86303493380546570000e-002) (13, -2.11602854728698730000e+000) (3, 1.47195637226104740000e-001) (4, 1.09319996833801270000e+000) (5, 1.35170590877532960000e+000) (6, 1.96820926666259770000e+000) (7, 8.71888256072998050000e+000) (8, -4.07386112213134770000e+000) (9, -1.76831841468811040000e+000) (10, 3.06489896774291990000e+000) (11, 5.77436971664428710000e+000) (12, 3.16783928871154790000e+000) (13, 2.42237091064453130000e-001) (3, 2.48722219467163090000e+000) (4, 5.22606372833251950000e+000) (5, -2.19328179955482480000e-002) (6, 1.27666771411895750000e+000) (7, -1.38023281097412110000e+000) (8, 4.46105861663818360000e+000) (9, 2.43034434318542480000e+000) (10, 1.76439809799194340000e+000) (11, -2.31356716156005860000e+000) (12, -1.30355989933013920000e+000) (13, -2.61897754669189450000e+000) (3, 6.53812170028686520000e-001) (4, 2.99741327762603760000e-001) (5, 1.09455132484436040000e+000) (6, 6.92010402679443360000e-001) (7, -4.53792047500610350000e+000) (8, 2.62647080421447750000e+000) (9, 2.91532325744628910000e+000) (10, 2.07745695114135740000e+000) (11, -2.87016220092773440000e+001) (12, -1.52398824691772460000e-001) (13, -1.76057088375091550000e+000) (3, -3.96884274482727050000e+000) (4, -2.11591005325317380000e+000) (5, 1.52875471115112300000e+000) (6, 2.79107642173767090000e+000) (7, 1.09797048568725590000e+001) (8, -7.03375673294067380000e+000) (9, -4.38773870468139650000e+000) (10, -6.59656450152397160000e-002) (11, 2.73025493621826170000e+001) (12, 2.77504277229309080000e+000) (13, -9.44496452808380130000e-001) (14, 1.95725834369659420000e+000) (15, -6.08922004699707030000e-001) (16, 1.12963581085205080000e+000) (17, 6.20653438568115230000e+000) (18, 3.13931918144226070000e+000) (19, 3.42568993568420410000e+000) (20, -9.69179809093475340000e-001) (21, 6.67942333221435550000e+000) (22, 2.06104469299316410000e+000) (23, -5.58597230911254880000e+000) (24, -2.15523338317871090000e+000) 
